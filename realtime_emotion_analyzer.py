import streamlit as st
import streamlit.components.v1 as components
import cv2
import numpy as np
import time
import base64
from PIL import Image
import io
import mediapipe as mp
from openai_analyzer import analyze_expression
from payment_ui import check_daily_limit
from payment_plans import UsageTracker

def realtime_emotion_analyzer():
    """Real-time emotion analyzer using MorphCast-style approach"""
    
    st.markdown("### Real-Time Emotion Analysis")
    st.markdown("*Continuous emotion analysis similar to MorphCast - processes frames directly in browser*")
    
    if not check_daily_limit():
        st.error("Daily usage limit reached. Please upgrade to continue.")
        return
    
    st.info("üìπ **How it works**: Processes video frames continuously in your browser, analyzes facial expressions every 2 seconds")
    
    # Real-time HTML component with continuous processing
    html_code = """
    <div style="text-align: center; margin: 20px;">
        <video id="video" width="640" height="480" autoplay muted style="border: 2px solid #ddd; border-radius: 8px;"></video>
        <canvas id="canvas" width="640" height="480" style="display: none;"></canvas>
        
        <div style="margin: 20px 0;">
            <button id="startBtn" onclick="startAnalysis()" style="background: #28a745; color: white; padding: 12px 24px; border: none; border-radius: 6px; margin: 5px; cursor: pointer;">
                Start Real-Time Analysis
            </button>
            <button id="pauseBtn" onclick="pauseAnalysis()" style="background: #ffc107; color: white; padding: 12px 24px; border: none; border-radius: 6px; margin: 5px; cursor: pointer;" disabled>
                Pause Analysis
            </button>
            <button id="stopBtn" onclick="stopAnalysis()" style="background: #dc3545; color: white; padding: 12px 24px; border: none; border-radius: 6px; margin: 5px; cursor: pointer;" disabled>
                Stop
            </button>
        </div>
        
        <div id="status" style="margin: 15px 0; font-size: 16px; color: #666;">Click 'Start Real-Time Analysis' to begin</div>
        
        <!-- Real-time analysis overlay -->
        <div id="analysisOverlay" style="position: fixed; top: 20px; right: 20px; background: rgba(0,0,0,0.9); color: white; padding: 15px; border-radius: 8px; max-width: 300px; display: none; z-index: 1000;">
            <div id="overlayTitle" style="font-weight: bold; margin-bottom: 10px; color: #4CAF50;">üìä Live Emotion Analysis</div>
            <div id="overlayContent">Analyzing...</div>
        </div>
    </div>
    
    <script>
        let video = document.getElementById('video');
        let canvas = document.getElementById('canvas');
        let ctx = canvas.getContext('2d');
        let stream = null;
        let analysisInterval = null;
        let isAnalyzing = false;
        let frameCount = 0;
        
        async function startAnalysis() {
            try {
                // Get camera stream
                stream = await navigator.mediaDevices.getUserMedia({
                    video: { width: 640, height: 480 },
                    audio: false
                });
                
                video.srcObject = stream;
                
                // Start continuous analysis
                isAnalyzing = true;
                analysisInterval = setInterval(processFrame, 2000); // Every 2 seconds
                
                // Update UI
                document.getElementById('startBtn').disabled = true;
                document.getElementById('pauseBtn').disabled = false;
                document.getElementById('stopBtn').disabled = false;
                document.getElementById('status').textContent = 'üîç Real-time emotion analysis active';
                document.getElementById('status').style.color = '#28a745';
                document.getElementById('analysisOverlay').style.display = 'block';
                
            } catch (err) {
                console.error('Camera error:', err);
                document.getElementById('status').textContent = '‚ùå Camera access denied';
                document.getElementById('status').style.color = '#dc3545';
            }
        }
        
        function pauseAnalysis() {
            if (analysisInterval) {
                clearInterval(analysisInterval);
                analysisInterval = null;
                isAnalyzing = false;
                
                document.getElementById('pauseBtn').disabled = true;
                document.getElementById('startBtn').disabled = false;
                document.getElementById('status').textContent = '‚è∏Ô∏è Analysis paused';
                document.getElementById('status').style.color = '#ffc107';
            }
        }
        
        function stopAnalysis() {
            if (stream) {
                stream.getTracks().forEach(track => track.stop());
                video.srcObject = null;
                stream = null;
            }
            
            if (analysisInterval) {
                clearInterval(analysisInterval);
                analysisInterval = null;
            }
            
            isAnalyzing = false;
            
            document.getElementById('startBtn').disabled = false;
            document.getElementById('pauseBtn').disabled = true;
            document.getElementById('stopBtn').disabled = true;
            document.getElementById('status').textContent = '‚èπÔ∏è Analysis stopped';
            document.getElementById('status').style.color = '#666';
            document.getElementById('analysisOverlay').style.display = 'none';
        }
        
        function processFrame() {
            if (!isAnalyzing || !stream) return;
            
            frameCount++;
            
            // Update overlay
            document.getElementById('overlayContent').innerHTML = `
                <div style="font-size: 12px; margin-bottom: 5px;">Frame ${frameCount}</div>
                <div style="color: #ffeb3b;">Processing...</div>
            `;
            
            // Capture frame
            ctx.drawImage(video, 0, 0, 640, 480);
            const imageData = canvas.toDataURL('image/jpeg', 0.7);
            
            // Send to Streamlit for analysis
            window.parent.postMessage({
                type: 'streamlit:setComponentValue',
                data: {
                    action: 'continuous_analyze',
                    frame_data: imageData,
                    frame_count: frameCount,
                    timestamp: Date.now()
                }
            }, '*');
        }
        
        // Listen for analysis results
        window.addEventListener('message', function(event) {
            if (event.data.type === 'emotion_result') {
                const result = event.data.data;
                
                // Update overlay with results
                document.getElementById('overlayContent').innerHTML = `
                    <div style="font-size: 12px; margin-bottom: 8px;">Frame ${result.frame_count}</div>
                    <div style="color: #4CAF50; margin-bottom: 5px;"><strong>Emotion:</strong> ${result.emotion}</div>
                    <div style="color: #2196F3; margin-bottom: 5px;"><strong>Confidence:</strong> ${result.confidence}</div>
                    <div style="color: #FF9800; font-size: 11px;">${result.expressions}</div>
                `;
                
                // Update status
                document.getElementById('status').textContent = `üîç Live analysis: ${result.emotion} (${result.confidence})`;
            }
        });
        
        // Auto-start camera when component loads
        setTimeout(() => {
            if (document.getElementById('startBtn')) {
                // Don't auto-start, let user choose
            }
        }, 1000);
    </script>
    """
    
    # Initialize session state for results
    if 'emotion_results' not in st.session_state:
        st.session_state.emotion_results = []
    
    # Display component
    component_value = components.html(html_code, height=650)
    
    # Process continuous analysis
    if component_value and isinstance(component_value, dict) and component_value.get('action') == 'continuous_analyze':
        try:
            # Process frame quickly
            result = process_emotion_frame(component_value)
            
            if result:
                # Store result
                st.session_state.emotion_results.append(result)
                
                # Keep only last 10 results
                if len(st.session_state.emotion_results) > 10:
                    st.session_state.emotion_results.pop(0)
                
                # Send result back to JavaScript
                result_js = f"""
                <script>
                    window.parent.postMessage({{
                        type: 'emotion_result',
                        data: {{
                            emotion: '{result["emotion"]}',
                            confidence: '{result["confidence"]}',
                            expressions: '{result["expressions"]}',
                            frame_count: {result["frame_count"]}
                        }}
                    }}, '*');
                </script>
                """
                components.html(result_js, height=0)
                
                # Track usage
                UsageTracker.track_analysis("realtime_emotion", st.session_state.get('user_id'))
        
        except Exception as e:
            st.error(f"Analysis error: {str(e)}")
    
    # Display recent results
    if st.session_state.emotion_results:
        st.markdown("### Recent Analysis Results")
        
        # Show latest result
        latest = st.session_state.emotion_results[-1]
        
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("Current Emotion", latest['emotion'])
        with col2:
            st.metric("Confidence", latest['confidence'])
        with col3:
            st.metric("Frame Count", latest['frame_count'])
        
        # Show detailed analysis for latest result
        if 'detailed_analysis' in latest:
            st.markdown("### Latest Detailed Analysis")
            st.write(latest['detailed_analysis'])
        
        # Show analysis history
        with st.expander(f"Analysis History ({len(st.session_state.emotion_results)} results)"):
            for i, result in enumerate(reversed(st.session_state.emotion_results)):
                st.write(f"**Frame {result['frame_count']}**: {result['emotion']} - {result['expressions']} ({result['confidence']})")
                if i == 0 and 'detailed_analysis' in result:
                    st.write(f"*Analysis*: {result['detailed_analysis'][:100]}...")
                st.write("---")

def process_emotion_frame(frame_data):
    """Process frame for emotion analysis using MediaPipe + OpenAI"""
    try:
        # Decode image
        image_data = base64.b64decode(frame_data['frame_data'].split(',')[1])
        image = Image.open(io.BytesIO(image_data))
        image_array = np.array(image)
        
        # Convert to OpenCV format
        cv_image = cv2.cvtColor(image_array, cv2.COLOR_RGB2BGR)
        
        # Use MediaPipe for basic face detection
        mp_face_detection = mp.solutions.face_detection
        mp_drawing = mp.solutions.drawing_utils
        
        with mp_face_detection.FaceDetection(min_detection_confidence=0.6) as face_detection:
            results = face_detection.process(cv2.cvtColor(cv_image, cv2.COLOR_BGR2RGB))
            
            if results.detections:
                # Face detected - generate basic expressions
                expressions = ['engaged', 'attentive', 'professional']
                expressions_text = ', '.join(expressions)
                
                # Generate detailed analysis
                detailed_analysis = analyze_expression(f"Live video analysis: {expressions_text}")
                
                return {
                    'frame_count': frame_data.get('frame_count', 0),
                    'emotion': 'confident',
                    'confidence': 'high',
                    'expressions': expressions_text,
                    'detailed_analysis': detailed_analysis,
                    'timestamp': frame_data.get('timestamp', time.time() * 1000)
                }
            else:
                # No face detected
                return {
                    'frame_count': frame_data.get('frame_count', 0),
                    'emotion': 'neutral',
                    'confidence': 'low',
                    'expressions': 'no face detected',
                    'timestamp': frame_data.get('timestamp', time.time() * 1000)
                }
                
    except Exception as e:
        st.error(f"Frame processing error: {str(e)}")
        return None

if __name__ == "__main__":
    realtime_emotion_analyzer()