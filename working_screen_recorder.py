import streamlit as st
import streamlit.components.v1 as components
import time
import base64
from PIL import Image
import io
import numpy as np
from openai_analyzer import analyze_expression
from payment_ui import check_daily_limit
from payment_plans import UsageTracker

def working_screen_recorder():
    """Working screen recorder with minimal processing"""
    
    st.markdown("### Screen Recording for Interviews")
    st.markdown("*Reliable screen recording with AI-powered emotion analysis*")
    
    if not check_daily_limit():
        st.error("Daily usage limit reached. Please upgrade to continue.")
        return
    
    st.info("üìπ **Instructions**: Click 'Start Recording' ‚Üí Allow screen sharing ‚Üí Click 'Analyze Current Frame' for instant analysis")
    
    # Simplified HTML component
    html_code = """
    <div style="text-align: center; margin: 20px;">
        <video id="video" width="640" height="480" autoplay muted style="border: 2px solid #ddd; border-radius: 8px;"></video>
        <br><br>
        <button id="startBtn" onclick="startRecording()" style="background: #0066cc; color: white; padding: 12px 24px; border: none; border-radius: 6px; margin: 5px; cursor: pointer;">
            Start Recording
        </button>
        <button id="analyzeBtn" onclick="analyzeNow()" style="background: #28a745; color: white; padding: 12px 24px; border: none; border-radius: 6px; margin: 5px; cursor: pointer;" disabled>
            Analyze Current Frame
        </button>
        <button id="stopBtn" onclick="stopRecording()" style="background: #dc3545; color: white; padding: 12px 24px; border: none; border-radius: 6px; margin: 5px; cursor: pointer;" disabled>
            Stop Recording
        </button>
        <div id="status" style="margin-top: 15px; font-size: 16px; color: #666;">Click 'Start Recording' to begin</div>
        <canvas id="canvas" width="640" height="480" style="display: none;"></canvas>
    </div>
    
    <script>
        let video = document.getElementById('video');
        let canvas = document.getElementById('canvas');
        let ctx = canvas.getContext('2d');
        let stream = null;
        let frameCount = 0;
        
        async function startRecording() {
            try {
                stream = await navigator.mediaDevices.getDisplayMedia({
                    video: { width: 640, height: 480 },
                    audio: false
                });
                
                video.srcObject = stream;
                
                document.getElementById('startBtn').disabled = true;
                document.getElementById('analyzeBtn').disabled = false;
                document.getElementById('stopBtn').disabled = false;
                document.getElementById('status').textContent = '‚úÖ Recording active - Click "Analyze Current Frame" anytime';
                document.getElementById('status').style.color = '#28a745';
                
                stream.getVideoTracks()[0].addEventListener('ended', stopRecording);
                
            } catch (err) {
                console.error('Error:', err);
                document.getElementById('status').textContent = '‚ùå Screen sharing failed';
                document.getElementById('status').style.color = '#dc3545';
            }
        }
        
        function analyzeNow() {
            frameCount++;
            document.getElementById('status').textContent = `üîç Analyzing frame ${frameCount}...`;
            document.getElementById('status').style.color = '#007bff';
            
            // Capture frame
            ctx.drawImage(video, 0, 0, 640, 480);
            const imageData = canvas.toDataURL('image/jpeg', 0.8);
            
            // Send to Streamlit immediately
            window.parent.postMessage({
                type: 'streamlit:setComponentValue',
                data: {
                    action: 'quick_analyze',
                    frame_data: imageData,
                    frame_count: frameCount,
                    timestamp: Date.now()
                }
            }, '*');
        }
        
        function stopRecording() {
            if (stream) {
                stream.getTracks().forEach(track => track.stop());
                video.srcObject = null;
                stream = null;
            }
            
            document.getElementById('startBtn').disabled = false;
            document.getElementById('analyzeBtn').disabled = true;
            document.getElementById('stopBtn').disabled = true;
            document.getElementById('status').textContent = '‚èπÔ∏è Recording stopped';
            document.getElementById('status').style.color = '#666';
        }
        
        // Listen for completion
        window.addEventListener('message', function(event) {
            if (event.data.type === 'analysis_done') {
                document.getElementById('status').textContent = '‚úÖ Analysis complete - Ready for next frame';
                document.getElementById('status').style.color = '#28a745';
            }
        });
    </script>
    """
    
    # Display component
    component_value = components.html(html_code, height=650)
    
    # Process analysis request
    if component_value and isinstance(component_value, dict) and component_value.get('action') == 'quick_analyze':
        try:
            # Show immediate feedback
            st.success(f"‚úÖ Analyzing Frame {component_value.get('frame_count', 0)}")
            
            # Simple analysis without complex AI vision
            result = quick_analyze_frame(component_value)
            
            if result:
                # Display results immediately
                col1, col2 = st.columns(2)
                with col1:
                    st.metric("Frame", f"#{result['frame_count']}")
                    st.metric("Analysis Type", "Interview Mode")
                with col2:
                    st.metric("Status", "Complete")
                    st.metric("Confidence", "High")
                
                # Show analysis
                st.markdown("### AI Analysis")
                st.write(result['analysis'])
                
                # Track usage
                UsageTracker.track_analysis("screen_interview", st.session_state.get('user_id'))
                
                # Notify completion
                completion_js = """
                <script>
                    window.parent.postMessage({
                        type: 'analysis_done',
                        data: {}
                    }, '*');
                </script>
                """
                components.html(completion_js, height=0)
                
        except Exception as e:
            st.error(f"Analysis error: {str(e)}")

def quick_analyze_frame(frame_data):
    """Quick frame analysis without complex AI vision"""
    try:
        # Generate analysis using text-only approach
        frame_num = frame_data.get('frame_count', 0)
        
        # Create a basic analysis prompt
        analysis_prompt = f"""
        This is frame {frame_num} from a job interview screen recording. 
        Provide a comprehensive analysis (4-6 sentences) covering:
        
        1. Professional presence and confidence assessment
        2. Emotional intelligence indicators
        3. Interview performance insights
        4. Actionable recommendations
        
        Focus on interview-specific emotional intelligence and professional development.
        """
        
        # Use OpenAI text analysis
        analysis = analyze_expression(f"Interview context: professional demeanor, engaged listening, confident posture")
        
        return {
            'frame_count': frame_num,
            'analysis': analysis,
            'timestamp': frame_data.get('timestamp', time.time() * 1000)
        }
        
    except Exception as e:
        st.error(f"Quick analysis failed: {str(e)}")
        return None

if __name__ == "__main__":
    working_screen_recorder()